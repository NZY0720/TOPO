#!/usr/bin/env python3
"""
å®éªŒè¿è¡Œè„šæœ¬ - æ”¯æŒå¤šç§å®éªŒé…ç½®å’Œæ‰¹é‡è¿è¡Œ
"""

import os
import sys
import argparse
import json
import time
from pathlib import Path
from typing import Dict, List, Any, Optional
import itertools

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

import torch
import numpy as np
from power_grid_topology.config.base_config import Config
from power_grid_topology.data.data_loader import PowerGridDataLoader
from power_grid_topology.training.trainer import PowerGridTrainer
from power_grid_topology.evaluation.evaluator import ModelEvaluator
from power_grid_topology.utils.io_utils import load_yaml, save_json
from power_grid_topology.utils.logging_utils import setup_logger


class ExperimentRunner:
    """å®éªŒè¿è¡Œå™¨"""
    
    def __init__(self, base_config: Config, base_output_dir: str):
        self.base_config = base_config
        self.base_output_dir = base_output_dir
        self.experiment_results = []
        
        # è®¾ç½®ä¸»æ—¥å¿—è®°å½•å™¨
        self.logger = setup_logger(
            'experiment_runner',
            os.path.join(base_output_dir, 'experiment_runner.log')
        )
    
    def run_single_experiment(self, config: Config, experiment_id: str) -> Dict[str, Any]:
        """è¿è¡Œå•ä¸ªå®éªŒ"""
        
        # è®¾ç½®å®éªŒç‰¹å®šçš„è¾“å‡ºç›®å½•
        exp_output_dir = os.path.join(self.base_output_dir, f"exp_{experiment_id}")
        config.experiment.output_dir = exp_output_dir
        config.experiment.log_dir = os.path.join(exp_output_dir, 'logs')
        config.experiment.checkpoint_dir = os.path.join(exp_output_dir, 'models')
        config.experiment.figure_dir = os.path.join(exp_output_dir, 'figures')
        config.experiment.name = f"experiment_{experiment_id}"
        
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(exp_output_dir, exist_ok=True)
        
        self.logger.info(f"ğŸš€ å¼€å§‹å®éªŒ {experiment_id}")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {exp_output_dir}")
        
        try:
            start_time = time.time()
            
            # 1. åŠ è½½æ•°æ®
            self.logger.info("ğŸ“‚ åŠ è½½æ•°æ®...")
            data_loader = PowerGridDataLoader(config.data)
            data_loader.load_data(config.data.data_path)
            data_loader.mask_unobservable_nodes(
                config.data.unobservable_ratio,
                seed=config.experiment.seed
            )
            
            # 2. åˆ›å»ºå’Œè®­ç»ƒæ¨¡å‹
            self.logger.info("ğŸ‹ï¸ å¼€å§‹è®­ç»ƒ...")
            trainer = PowerGridTrainer(config)
            trainer.train(data_loader)
            
            # 3. è¯„ä¼°æ¨¡å‹
            self.logger.info("ğŸ“Š å¼€å§‹è¯„ä¼°...")
            evaluator = ModelEvaluator(config)
            
            # åŠ è½½æœ€ä½³æ¨¡å‹
            best_model_path = os.path.join(config.experiment.checkpoint_dir, 'best_model.pth')
            if os.path.exists(best_model_path):
                from power_grid_topology.models.graph_transformer import create_model
                
                sample_data = data_loader.create_graph_data(0)
                input_dim = sample_data.x.size(1)
                model = create_model(config.model, input_dim)
                model = evaluator.load_model(best_model_path, model)
            else:
                self.logger.warning("æœ€ä½³æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½¿ç”¨æœ€ç»ˆæ¨¡å‹")
                model = trainer.model
            
            # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
            report_path = evaluator.generate_evaluation_report(model, data_loader)
            
            # 4. æ”¶é›†ç»“æœ
            training_time = time.time() - start_time
            
            # è·å–æœ€ç»ˆæŒ‡æ ‡
            final_metrics = {}
            if hasattr(trainer, 'training_history') and trainer.training_history['metrics']:
                final_metrics = trainer.training_history['metrics'][-1]
            
            # å®éªŒç»“æœ
            experiment_result = {
                'experiment_id': experiment_id,
                'config': config.to_dict(),
                'training_time': training_time,
                'final_metrics': final_metrics,
                'best_val_loss': trainer.best_val_loss,
                'total_epochs': trainer.current_epoch + 1,
                'output_dir': exp_output_dir,
                'report_path': report_path,
                'status': 'completed'
            }
            
            # ä¿å­˜å®éªŒç»“æœ
            result_path = os.path.join(exp_output_dir, 'experiment_result.json')
            save_json(experiment_result, result_path)
            
            self.logger.info(f"âœ… å®éªŒ {experiment_id} å®Œæˆï¼Œè€—æ—¶: {training_time:.2f}ç§’")
            
            return experiment_result
            
        except Exception as e:
            self.logger.error(f"âŒ å®éªŒ {experiment_id} å¤±è´¥: {e}", exc_info=True)
            
            # è¿”å›å¤±è´¥ç»“æœ
            return {
                'experiment_id': experiment_id,
                'config': config.to_dict(),
                'status': 'failed',
                'error': str(e),
                'output_dir': exp_output_dir
            }
    
    def run_grid_search(self, param_grid: Dict[str, List[Any]], 
                       data_path: str, max_experiments: Optional[int] = None) -> List[Dict[str, Any]]:
        """è¿è¡Œç½‘æ ¼æœç´¢å®éªŒ"""
        
        self.logger.info("ğŸ” å¼€å§‹ç½‘æ ¼æœç´¢å®éªŒ")
        self.logger.info(f"å‚æ•°ç½‘æ ¼: {param_grid}")
        
        # ç”Ÿæˆæ‰€æœ‰å‚æ•°ç»„åˆ
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        param_combinations = list(itertools.product(*param_values))
        
        if max_experiments and len(param_combinations) > max_experiments:
            # éšæœºé‡‡æ ·
            import random
            param_combinations = random.sample(param_combinations, max_experiments)
            self.logger.info(f"éšæœºé‡‡æ · {max_experiments} ä¸ªå®éªŒé…ç½®")
        
        self.logger.info(f"æ€»å®éªŒæ•°é‡: {len(param_combinations)}")
        
        results = []
        
        for i, param_values in enumerate(param_combinations):
            # åˆ›å»ºå®éªŒé…ç½®
            config = Config()
            config.__dict__.update(self.base_config.__dict__)
            config.data.data_path = data_path
            
            # è®¾ç½®å‚æ•°
            for param_name, param_value in zip(param_names, param_values):
                self._set_nested_param(config, param_name, param_value)
            
            # è®¾ç½®å®éªŒID
            experiment_id = f"grid_search_{i:03d}"
            
            # è¿è¡Œå®éªŒ
            result = self.run_single_experiment(config, experiment_id)
            results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            self.experiment_results = results
            self._save_experiment_summary()
        
        self.logger.info(f"ğŸ‰ ç½‘æ ¼æœç´¢å®Œæˆï¼Œå…±è¿è¡Œ {len(results)} ä¸ªå®éªŒ")
        return results
    
    def run_random_search(self, param_distributions: Dict[str, Any], 
                         data_path: str, n_experiments: int = 10) -> List[Dict[str, Any]]:
        """è¿è¡Œéšæœºæœç´¢å®éªŒ"""
        
        self.logger.info("ğŸ² å¼€å§‹éšæœºæœç´¢å®éªŒ")
        self.logger.info(f"å®éªŒæ•°é‡: {n_experiments}")
        
        results = []
        
        for i in range(n_experiments):
            # éšæœºé‡‡æ ·å‚æ•°
            config = Config()
            config.__dict__.update(self.base_config.__dict__)
            config.data.data_path = data_path
            
            for param_name, distribution in param_distributions.items():
                if isinstance(distribution, list):
                    # ä»åˆ—è¡¨ä¸­éšæœºé€‰æ‹©
                    param_value = np.random.choice(distribution)
                elif isinstance(distribution, dict):
                    # æ ¹æ®åˆ†å¸ƒç±»å‹é‡‡æ ·
                    if distribution['type'] == 'uniform':
                        param_value = np.random.uniform(distribution['low'], distribution['high'])
                    elif distribution['type'] == 'log_uniform':
                        param_value = np.exp(np.random.uniform(
                            np.log(distribution['low']), np.log(distribution['high'])
                        ))
                    elif distribution['type'] == 'choice':
                        param_value = np.random.choice(distribution['choices'])
                    else:
                        raise ValueError(f"æœªçŸ¥çš„åˆ†å¸ƒç±»å‹: {distribution['type']}")
                else:
                    param_value = distribution
                
                self._set_nested_param(config, param_name, param_value)
            
            # è®¾ç½®å®éªŒID
            experiment_id = f"random_search_{i:03d}"
            
            # è¿è¡Œå®éªŒ
            result = self.run_single_experiment(config, experiment_id)
            results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            self.experiment_results = results
            self._save_experiment_summary()
        
        self.logger.info(f"ğŸ‰ éšæœºæœç´¢å®Œæˆï¼Œå…±è¿è¡Œ {len(results)} ä¸ªå®éªŒ")
        return results
    
    def _set_nested_param(self, config: Config, param_path: str, value: Any):
        """è®¾ç½®åµŒå¥—å‚æ•°"""
        parts = param_path.split('.')
        obj = config
        
        for part in parts[:-1]:
            obj = getattr(obj, part)
        
        setattr(obj, parts[-1], value)
    
    def _save_experiment_summary(self):
        """ä¿å­˜å®éªŒæ‘˜è¦"""
        summary_path = os.path.join(self.base_output_dir, 'experiment_summary.json')
        
        summary = {
            'total_experiments': len(self.experiment_results),
            'completed_experiments': len([r for r in self.experiment_results if r['status'] == 'completed']),
            'failed_experiments': len([r for r in self.experiment_results if r['status'] == 'failed']),
            'results': self.experiment_results
        }
        
        save_json(summary, summary_path)
    
    def analyze_results(self) -> Dict[str, Any]:
        """åˆ†æå®éªŒç»“æœ"""
        if not self.experiment_results:
            return {}
        
        completed_results = [r for r in self.experiment_results if r['status'] == 'completed']
        
        if not completed_results:
            return {'message': 'æ²¡æœ‰æˆåŠŸå®Œæˆçš„å®éªŒ'}
        
        # æå–å…³é”®æŒ‡æ ‡
        f1_scores = []
        training_times = []
        val_losses = []
        
        for result in completed_results:
            final_metrics = result.get('final_metrics', {})
            topo_metrics = final_metrics.get('topology', {})
            
            f1_scores.append(topo_metrics.get('f1_score', 0))
            training_times.append(result.get('training_time', 0))
            val_losses.append(result.get('best_val_loss', float('inf')))
        
        # æ‰¾åˆ°æœ€ä½³å®éªŒ
        best_f1_idx = np.argmax(f1_scores)
        best_experiment = completed_results[best_f1_idx]
        
        analysis = {
            'total_experiments': len(self.experiment_results),
            'completed_experiments': len(completed_results),
            'best_experiment': {
                'experiment_id': best_experiment['experiment_id'],
                'f1_score': f1_scores[best_f1_idx],
                'training_time': best_experiment['training_time'],
                'output_dir': best_experiment['output_dir']
            },
            'statistics': {
                'f1_score': {
                    'mean': float(np.mean(f1_scores)),
                    'std': float(np.std(f1_scores)),
                    'min': float(np.min(f1_scores)),
                    'max': float(np.max(f1_scores))
                },
                'training_time': {
                    'mean': float(np.mean(training_times)),
                    'std': float(np.std(training_times)),
                    'min': float(np.min(training_times)),
                    'max': float(np.max(training_times))
                }
            }
        }
        
        # ä¿å­˜åˆ†æç»“æœ
        analysis_path = os.path.join(self.base_output_dir, 'experiment_analysis.json')
        save_json(analysis, analysis_path)
        
        return analysis


def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="è¿è¡Œç”µåŠ›ç½‘æ ¼æ‹“æ‰‘é‡å»ºå®éªŒ")
    
    parser.add_argument('--config', type=str, required=True,
                       help='åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--data_path', type=str, required=True,
                       help='æ•°æ®è·¯å¾„')
    parser.add_argument('--output_dir', type=str, default='./experiments_output',
                       help='å®éªŒè¾“å‡ºç›®å½•')
    parser.add_argument('--experiment_type', type=str, 
                       choices=['single', 'grid_search', 'random_search'],
                       default='single', help='å®éªŒç±»å‹')
    parser.add_argument('--param_config', type=str, default=None,
                       help='å‚æ•°é…ç½®æ–‡ä»¶ï¼ˆç”¨äºç½‘æ ¼æœç´¢å’Œéšæœºæœç´¢ï¼‰')
    parser.add_argument('--max_experiments', type=int, default=None,
                       help='æœ€å¤§å®éªŒæ•°é‡')
    parser.add_argument('--n_random', type=int, default=10,
                       help='éšæœºæœç´¢å®éªŒæ•°é‡')
    parser.add_argument('--seed', type=int, default=42,
                       help='éšæœºç§å­')
    
    return parser.parse_args()


def main():
    """ä¸»å‡½æ•°"""
    args = parse_arguments()
    
    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(args.seed)
    np.random.seed(args.seed)
    
    # åŠ è½½åŸºç¡€é…ç½®
    print(f"ğŸ“„ åŠ è½½é…ç½®æ–‡ä»¶: {args.config}")
    config_dict = load_yaml(args.config)
    base_config = Config.from_dict(config_dict)
    base_config.experiment.seed = args.seed
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(args.output_dir, exist_ok=True)
    
    # åˆ›å»ºå®éªŒè¿è¡Œå™¨
    runner = ExperimentRunner(base_config, args.output_dir)
    
    print(f"ğŸ§ª å®éªŒç±»å‹: {args.experiment_type}")
    print(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
    
    try:
        if args.experiment_type == 'single':
            # å•ä¸ªå®éªŒ
            base_config.data.data_path = args.data_path
            result = runner.run_single_experiment(base_config, "single")
            
            if result['status'] == 'completed':
                print(f"âœ… å®éªŒå®Œæˆ!")
                print(f"ğŸ“Š æœ€ç»ˆF1åˆ†æ•°: {result['final_metrics'].get('topology', {}).get('f1_score', 0):.3f}")
            else:
                print(f"âŒ å®éªŒå¤±è´¥: {result.get('error', 'Unknown error')}")
        
        elif args.experiment_type == 'grid_search':
            # ç½‘æ ¼æœç´¢
            if not args.param_config:
                raise ValueError("ç½‘æ ¼æœç´¢éœ€è¦å‚æ•°é…ç½®æ–‡ä»¶")
            
            param_config = load_yaml(args.param_config)
            param_grid = param_config.get('grid_search', {})
            
            results = runner.run_grid_search(
                param_grid, args.data_path, args.max_experiments
            )
            
            # åˆ†æç»“æœ
            analysis = runner.analyze_results()
            print(f"ğŸ‰ ç½‘æ ¼æœç´¢å®Œæˆ!")
            print(f"ğŸ“Š æœ€ä½³F1åˆ†æ•°: {analysis['best_experiment']['f1_score']:.3f}")
            print(f"ğŸ† æœ€ä½³å®éªŒ: {analysis['best_experiment']['experiment_id']}")
        
        elif args.experiment_type == 'random_search':
            # éšæœºæœç´¢
            if not args.param_config:
                raise ValueError("éšæœºæœç´¢éœ€è¦å‚æ•°é…ç½®æ–‡ä»¶")
            
            param_config = load_yaml(args.param_config)
            param_distributions = param_config.get('random_search', {})
            
            results = runner.run_random_search(
                param_distributions, args.data_path, args.n_random
            )
            
            # åˆ†æç»“æœ
            analysis = runner.analyze_results()
            print(f"ğŸ‰ éšæœºæœç´¢å®Œæˆ!")
            print(f"ğŸ“Š æœ€ä½³F1åˆ†æ•°: {analysis['best_experiment']['f1_score']:.3f}")
            print(f"ğŸ† æœ€ä½³å®éªŒ: {analysis['best_experiment']['experiment_id']}")
        
        print(f"\nğŸ“ æ‰€æœ‰ç»“æœä¿å­˜åœ¨: {args.output_dir}")
        
    except KeyboardInterrupt:
        print("\nâš ï¸  å®éªŒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ å®éªŒè¿è¡Œå¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    main()